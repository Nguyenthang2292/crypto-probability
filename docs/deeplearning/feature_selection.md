# üìö Feature Selection Documentation

## M·ª•c l·ª•c
1. [T·ªïng quan](#t·ªïng-quan)
2. [Kh·ªüi t·∫°o](#kh·ªüi-t·∫°o)
3. [Ph∆∞∆°ng th·ª©c](#ph∆∞∆°ng-th·ª©c)
4. [V√≠ d·ª• s·ª≠ d·ª•ng](#v√≠-d·ª•-s·ª≠-d·ª•ng)
5. [Best Practices](#best-practices)
6. [C√°c ph∆∞∆°ng ph√°p Feature Selection](#c√°c-ph∆∞∆°ng-ph√°p-feature-selection)

---

## T·ªïng quan

`FeatureSelector` l√† m·ªôt module to√†n di·ªán ƒë·ªÉ ch·ªçn l·ªçc v√† k·ªπ thu·∫≠t h√≥a features cho deep learning models. Module n√†y cung c·∫•p:

- ‚úÖ **Mutual Information Selection** - Ch·ªçn features d·ª±a tr√™n mutual information v·ªõi target
- ‚úÖ **Boruta-like Selection** - S·ª≠ d·ª•ng Random Forest importance ƒë·ªÉ ch·ªçn features
- ‚úÖ **F-test Selection** - S·ª≠ d·ª•ng ANOVA F-statistic
- ‚úÖ **Combined Method** - K·∫øt h·ª£p Mutual Information v√† Boruta
- ‚úÖ **Collinearity Removal** - Lo·∫°i b·ªè features c√≥ correlation cao ƒë·ªÉ c·∫£i thi·ªán t√≠nh ·ªïn ƒë·ªãnh c·ªßa model
- ‚úÖ **Feature Filtering** - T·ª± ƒë·ªông lo·∫°i b·ªè invalid features (non-numeric, constant, target leakage)
- ‚úÖ **Persistent Storage** - L∆∞u v√† t·∫£i k·∫øt qu·∫£ feature selection ƒë·ªÉ t√°i s·ª≠ d·ª•ng

### Khi n√†o d√πng FeatureSelector?

| M·ª•c ƒë√≠ch | D√πng FeatureSelector? | Ph∆∞∆°ng th·ª©c |
|----------|----------------------|-------------|
| Ch·ªçn top 20-30 features quan tr·ªçng nh·∫•t | ‚úÖ C√≥ | `select_features()` |
| Lo·∫°i b·ªè features c√≥ correlation cao | ‚úÖ C√≥ | T·ª± ƒë·ªông trong `select_features()` |
| Tr√°nh "Garbage In, Garbage Out" | ‚úÖ C√≥ | T·ª± ƒë·ªông filter invalid features |
| L∆∞u k·∫øt qu·∫£ ƒë·ªÉ t√°i s·ª≠ d·ª•ng | ‚úÖ C√≥ | `_save_selection()` / `load_selection()` |
| √Åp d·ª•ng selection ƒë√£ l∆∞u cho data m·ªõi | ‚úÖ C√≥ | `apply_selection()` |
| Xem feature importance scores | ‚úÖ C√≥ | `get_feature_importance_report()` |

---

## Kh·ªüi t·∫°o

### C√∫ ph√°p

```python
from modules.feature_selection import FeatureSelector

selector = FeatureSelector(
    method="mutual_info",  # 'mutual_info', 'boruta', 'f_test', or 'combined'
    top_k=25,  # S·ªë l∆∞·ª£ng features c·∫ßn ch·ªçn
    collinearity_threshold=0.85,  # Ng∆∞·ª°ng correlation ƒë·ªÉ lo·∫°i b·ªè
    selection_dir="artifacts/deep/feature_selection"  # Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£
)
```

### Tham s·ªë

- `method` (str, **m·∫∑c ƒë·ªãnh**: `"mutual_info"`): Ph∆∞∆°ng ph√°p ch·ªçn features
  - `"mutual_info"`: Mutual Information (khuy·∫øn ngh·ªã cho h·∫ßu h·∫øt tr∆∞·ªùng h·ª£p)
  - `"boruta"`: Random Forest importance (t·ªët cho non-linear relationships)
  - `"f_test"`: ANOVA F-statistic (nhanh, t·ªët cho linear relationships)
  - `"combined"`: K·∫øt h·ª£p Mutual Information v√† Boruta
- `top_k` (int, **m·∫∑c ƒë·ªãnh**: `25`): S·ªë l∆∞·ª£ng top features c·∫ßn ch·ªçn (20-30 khuy·∫øn ngh·ªã)
- `collinearity_threshold` (float, **m·∫∑c ƒë·ªãnh**: `0.85`): Ng∆∞·ª°ng correlation ƒë·ªÉ lo·∫°i b·ªè collinear features (0.8-0.95)
- `selection_dir` (str, **m·∫∑c ƒë·ªãnh**: `"artifacts/deep/feature_selection"`): Th∆∞ m·ª•c l∆∞u/load k·∫øt qu·∫£

### V√≠ d·ª• kh·ªüi t·∫°o

```python
from modules.feature_selection import FeatureSelector

# C√°ch 1: S·ª≠ d·ª•ng m·∫∑c ƒë·ªãnh (mutual_info, top 25)
selector = FeatureSelector()

# C√°ch 2: T√πy ch·ªânh ph∆∞∆°ng ph√°p v√† s·ªë l∆∞·ª£ng
selector = FeatureSelector(
    method="boruta",
    top_k=30,
    collinearity_threshold=0.9
)

# C√°ch 3: S·ª≠ d·ª•ng combined method
selector = FeatureSelector(
    method="combined",
    top_k=20
)
```

### Thu·ªôc t√≠nh

Sau khi kh·ªüi t·∫°o, `FeatureSelector` c√≥ c√°c thu·ªôc t√≠nh:

- `method`: Ph∆∞∆°ng ph√°p ch·ªçn features ƒë√£ ch·ªçn
- `top_k`: S·ªë l∆∞·ª£ng features c·∫ßn ch·ªçn
- `collinearity_threshold`: Ng∆∞·ª°ng correlation
- `selected_features`: Danh s√°ch features ƒë√£ ch·ªçn (sau khi g·ªçi `select_features()`)
- `feature_scores`: Dictionary ch·ª©a scores c·ªßa t·∫•t c·∫£ features
- `selection_metadata`: Metadata c·ªßa selection (method, top_k, etc.)

---

## Ph∆∞∆°ng th·ª©c

### `select_features(X, y, task_type="regression", symbol=None) -> pd.DataFrame`

Ch·ªçn top features s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p ƒë√£ ch·ªâ ƒë·ªãnh.

**Tham s·ªë:**
- `X` (pd.DataFrame): DataFrame ch·ª©a features
- `y` (pd.Series): Target Series (continuous cho regression, discrete cho classification)
- `task_type` (str, **m·∫∑c ƒë·ªãnh**: `"regression"`): Lo·∫°i task - `"regression"` ho·∫∑c `"classification"`
- `symbol` (str, **t√πy ch·ªçn**): T√™n symbol ƒë·ªÉ l∆∞u selection per-symbol

**Tr·∫£ v·ªÅ:**
- `pd.DataFrame`: DataFrame ch·ªâ ch·ª©a c√°c features ƒë√£ ch·ªçn

**Quy tr√¨nh:**
1. **Filter invalid features**: Lo·∫°i b·ªè non-numeric, constant, target leakage columns
2. **Remove collinear features**: Lo·∫°i b·ªè features c√≥ correlation > threshold
3. **Apply selection method**: Ch·ªçn top K features
4. **Save results**: L∆∞u k·∫øt qu·∫£ v√†o disk

**V√≠ d·ª•:**

```python
import pandas as pd
from modules.feature_selection import FeatureSelector

# T·∫°o selector
selector = FeatureSelector(method="mutual_info", top_k=25)

# Ch·ªçn features
X_selected = selector.select_features(
    X=train_features_df,
    y=train_target_series,
    task_type="regression",
    symbol="BTC/USDT"
)

print(f"Selected {len(selector.selected_features)} features")
print(selector.selected_features)
```

### `apply_selection(X) -> pd.DataFrame`

√Åp d·ª•ng selection ƒë√£ c√≥ (t·ª´ `select_features()` ho·∫∑c `load_selection()`) cho DataFrame m·ªõi.

**Tham s·ªë:**
- `X` (pd.DataFrame): DataFrame m·ªõi c·∫ßn √°p d·ª•ng selection

**Tr·∫£ v·ªÅ:**
- `pd.DataFrame`: DataFrame ch·ªâ ch·ª©a selected features

**L∆∞u √Ω:** Ph·∫£i g·ªçi `select_features()` ho·∫∑c `load_selection()` tr∆∞·ªõc.

**V√≠ d·ª•:**

```python
# ƒê√£ c√≥ selection t·ª´ tr∆∞·ªõc
selector.load_selection(symbol="BTC/USDT")

# √Åp d·ª•ng cho validation/test data
X_val_selected = selector.apply_selection(X_val)
X_test_selected = selector.apply_selection(X_test)
```

### `load_selection(symbol=None) -> Optional[Dict]`

Load k·∫øt qu·∫£ feature selection ƒë√£ l∆∞u t·ª´ disk.

**Tham s·ªë:**
- `symbol` (str, **t√πy ch·ªçn**): T√™n symbol (n·∫øu l∆∞u per-symbol)

**Tr·∫£ v·ªÅ:**
- `Optional[Dict]`: Metadata c·ªßa selection ho·∫∑c `None` n·∫øu kh√¥ng t√¨m th·∫•y

**V√≠ d·ª•:**

```python
metadata = selector.load_selection(symbol="BTC/USDT")
if metadata:
    print(f"Loaded {len(selector.selected_features)} features")
    print(f"Method: {metadata['method']}")
```

### `get_feature_importance_report() -> pd.DataFrame`

L·∫•y b√°o c√°o feature importance scores.

**Tr·∫£ v·ªÅ:**
- `pd.DataFrame`: DataFrame v·ªõi columns: `feature`, `score`, `selected`

**V√≠ d·ª•:**

```python
report = selector.get_feature_importance_report()
print(report.head(10))  # Top 10 features

# L·ªçc ch·ªâ selected features
selected_report = report[report["selected"] == True]
print(selected_report)
```

---

## V√≠ d·ª• s·ª≠ d·ª•ng

### V√≠ d·ª• 1: Basic Feature Selection

```python
from modules.feature_selection import FeatureSelector
import pandas as pd

# Kh·ªüi t·∫°o
selector = FeatureSelector(
    method="mutual_info",
    top_k=25,
    collinearity_threshold=0.85
)

# Ch·ªçn features
X_selected = selector.select_features(
    X=train_X,
    y=train_y,
    task_type="regression"
)

# Xem k·∫øt qu·∫£
print(f"Selected {len(selector.selected_features)} features")
print(selector.selected_features[:10])  # Top 10
```

### V√≠ d·ª• 2: S·ª≠ d·ª•ng v·ªõi Classification

```python
selector = FeatureSelector(method="boruta", top_k=30)

# Classification task
X_selected = selector.select_features(
    X=train_X,
    y=train_labels,  # Categorical labels
    task_type="classification",
    symbol="BTC/USDT"
)

# Xem importance report
report = selector.get_feature_importance_report()
print(report.sort_values("score", ascending=False).head(15))
```

### V√≠ d·ª• 3: Load v√† Apply Selection

```python
# Load selection ƒë√£ l∆∞u
selector = FeatureSelector()
metadata = selector.load_selection(symbol="BTC/USDT")

if metadata:
    # √Åp d·ª•ng cho validation v√† test sets
    X_val_selected = selector.apply_selection(X_val)
    X_test_selected = selector.apply_selection(X_test)
else:
    print("No saved selection found. Run select_features() first.")
```

### V√≠ d·ª• 4: Combined Method

```python
# S·ª≠ d·ª•ng combined method (Mutual Info + Boruta)
selector = FeatureSelector(
    method="combined",
    top_k=20
)

X_selected = selector.select_features(
    X=train_X,
    y=train_y,
    task_type="regression"
)

# Combined method s·∫Ω normalize v√† average scores t·ª´ c·∫£ 2 methods
```

---

## Best Practices

### 1. Ch·ªçn ph∆∞∆°ng ph√°p ph√π h·ª£p

| Ph∆∞∆°ng ph√°p | Khi n√†o d√πng | ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm |
|------------|--------------|---------|------------|
| `mutual_info` | H·∫ßu h·∫øt tr∆∞·ªùng h·ª£p | Kh√¥ng gi·∫£ ƒë·ªãnh linear, nhanh | C√≥ th·ªÉ miss non-linear patterns ph·ª©c t·∫°p |
| `boruta` | Non-linear relationships | Ph√°t hi·ªán interactions t·ªët | Ch·∫≠m h∆°n (c·∫ßn train RF) |
| `f_test` | Linear relationships | R·∫•t nhanh | Ch·ªâ ph√°t hi·ªán linear relationships |
| `combined` | Mu·ªën k·∫øt h·ª£p ∆∞u ƒëi·ªÉm | C√¢n b·∫±ng gi·ªØa c√°c methods | Ch·∫≠m nh·∫•t |

### 2. S·ªë l∆∞·ª£ng features (top_k)

- **20-30 features**: Khuy·∫øn ngh·ªã cho h·∫ßu h·∫øt tr∆∞·ªùng h·ª£p
- **< 20**: C√≥ th·ªÉ miss important features
- **> 30**: C√≥ th·ªÉ g√¢y overfitting, tƒÉng training time

### 3. Collinearity threshold

- **0.85 (m·∫∑c ƒë·ªãnh)**: C√¢n b·∫±ng t·ªët
- **0.8**: Lo·∫°i b·ªè nhi·ªÅu h∆°n (n·∫øu c√≥ qu√° nhi·ªÅu correlated features)
- **0.9-0.95**: Ch·ªâ lo·∫°i b·ªè highly correlated (gi·ªØ l·∫°i nhi·ªÅu features h∆°n)

### 4. Per-symbol selection

N·∫øu training multi-asset, n√™n l∆∞u selection per-symbol:

```python
for symbol in symbols:
    selector.select_features(
        X=symbol_X,
        y=symbol_y,
        symbol=symbol  # L∆∞u per-symbol
    )
```

### 5. Validation workflow

```python
# 1. Select tr√™n training set
X_train_selected = selector.select_features(X_train, y_train)

# 2. L∆∞u selection
# (T·ª± ƒë·ªông trong select_features)

# 3. Load v√† apply cho validation/test
selector.load_selection()
X_val_selected = selector.apply_selection(X_val)
X_test_selected = selector.apply_selection(X_test)
```

---

## C√°c ph∆∞∆°ng ph√°p Feature Selection

### 1. Mutual Information

**C√°ch ho·∫°t ƒë·ªông:**
- ƒêo l∆∞·ªùng mutual information gi·ªØa m·ªói feature v√† target
- Ch·ªçn K features c√≥ mutual information cao nh·∫•t
- Kh√¥ng gi·∫£ ƒë·ªãnh linear relationship

**∆Øu ƒëi·ªÉm:**
- Nhanh
- Ph√°t hi·ªán c·∫£ linear v√† non-linear relationships
- Kh√¥ng c·∫ßn train model

**Nh∆∞·ª£c ƒëi·ªÉm:**
- C√≥ th·ªÉ miss complex interactions

### 2. Boruta-like (Random Forest)

**C√°ch ho·∫°t ƒë·ªông:**
- Train Random Forest model
- S·ª≠ d·ª•ng feature importance scores
- Ch·ªçn K features c√≥ importance cao nh·∫•t

**∆Øu ƒëi·ªÉm:**
- Ph√°t hi·ªán non-linear relationships t·ªët
- Ph√°t hi·ªán feature interactions
- Robust v·ªõi noise

**Nh∆∞·ª£c ƒëi·ªÉm:**
- Ch·∫≠m h∆°n (c·∫ßn train RF)
- C√≥ th·ªÉ overfit n·∫øu RF overfit

### 3. F-test (ANOVA)

**C√°ch ho·∫°t ƒë·ªông:**
- T√≠nh F-statistic (ANOVA) gi·ªØa features v√† target
- Ch·ªçn K features c√≥ F-statistic cao nh·∫•t
- Gi·∫£ ƒë·ªãnh linear relationship

**∆Øu ƒëi·ªÉm:**
- R·∫•t nhanh
- T·ªët cho linear relationships

**Nh∆∞·ª£c ƒëi·ªÉm:**
- Ch·ªâ ph√°t hi·ªán linear relationships
- Miss non-linear patterns

### 4. Combined

**C√°ch ho·∫°t ƒë·ªông:**
- Ch·∫°y c·∫£ Mutual Information v√† Boruta
- Normalize scores t·ª´ c·∫£ 2 methods
- Average normalized scores
- Ch·ªçn K features c√≥ combined score cao nh·∫•t

**∆Øu ƒëi·ªÉm:**
- K·∫øt h·ª£p ∆∞u ƒëi·ªÉm c·ªßa c·∫£ 2 methods
- Robust h∆°n

**Nh∆∞·ª£c ƒëi·ªÉm:**
- Ch·∫≠m nh·∫•t (c·∫ßn train RF + t√≠nh MI)

---

## T·ª± ƒë·ªông Filter Invalid Features

Module t·ª± ƒë·ªông lo·∫°i b·ªè:

1. **Non-numeric columns**: Ch·ªâ gi·ªØ numeric features
2. **Columns v·ªõi >50% NaN**: Lo·∫°i b·ªè columns c√≥ qu√° nhi·ªÅu missing values
3. **Constant columns**: Lo·∫°i b·ªè columns c√≥ zero variance
4. **Target leakage columns**: Lo·∫°i b·ªè columns ch·ª©a future information
   - Columns c√≥ ch·ª©a: `future_`, `target`, `label`, `triple_barrier`
5. **Metadata columns**: Lo·∫°i b·ªè `timestamp`, `symbol`, `time_idx`

---

## L∆∞u v√† Load Selection

### L∆∞u t·ª± ƒë·ªông

Khi g·ªçi `select_features()`, k·∫øt qu·∫£ t·ª± ƒë·ªông ƒë∆∞·ª£c l∆∞u v√†o:
```
artifacts/deep/feature_selection/feature_selection_{symbol}.json
```

### Load selection

```python
selector = FeatureSelector()
metadata = selector.load_selection(symbol="BTC/USDT")

# Sau khi load, c√≥ th·ªÉ d√πng apply_selection()
X_new_selected = selector.apply_selection(X_new)
```

### Format c·ªßa saved file

```json
{
  "method": "mutual_info",
  "top_k": 25,
  "collinearity_threshold": 0.85,
  "selected_features": ["feature1", "feature2", ...],
  "feature_scores": {
    "feature1": 0.85,
    "feature2": 0.72,
    ...
  }
}
```

---

## T√≠ch h·ª£p v·ªõi DeepLearningDataPipeline

Feature selection ƒë∆∞·ª£c t√≠ch h·ª£p t·ª± ƒë·ªông trong `DeepLearningDataPipeline`:

```python
from modules.deeplearning_data_pipeline import DeepLearningDataPipeline

# Feature selection t·ª± ƒë·ªông ƒë∆∞·ª£c √°p d·ª•ng trong split_chronological()
pipeline = DeepLearningDataPipeline(data_fetcher)
df = pipeline.fetch_and_prepare(symbols=["BTC/USDT"])
train_df, val_df, test_df = pipeline.split_chronological(
    df,
    apply_feature_selection=True  # M·∫∑c ƒë·ªãnh True
)
```

---

## Troubleshooting

### L·ªói: "No valid features after filtering"

**Nguy√™n nh√¢n:** T·∫•t c·∫£ features ƒë·ªÅu b·ªã lo·∫°i b·ªè (constant, NaN, etc.)

**Gi·∫£i ph√°p:**
- Ki·ªÉm tra data quality
- Gi·∫£m `nan_threshold` trong `_filter_invalid_features()`
- Ki·ªÉm tra c√≥ features n√†o valid kh√¥ng

### L·ªói: "No features selected"

**Nguy√™n nh√¢n:** Ch∆∞a g·ªçi `select_features()` ho·∫∑c `load_selection()`

**Gi·∫£i ph√°p:**
```python
# Ph·∫£i g·ªçi select_features() tr∆∞·ªõc
selector.select_features(X, y)
# Sau ƒë√≥ m·ªõi d√πng apply_selection()
X_selected = selector.apply_selection(X_new)
```

### Selection qu√° √≠t features

**Nguy√™n nh√¢n:** `top_k` qu√° nh·ªè ho·∫∑c `collinearity_threshold` qu√° cao

**Gi·∫£i ph√°p:**
- TƒÉng `top_k` (v√≠ d·ª•: 25 ‚Üí 30)
- Gi·∫£m `collinearity_threshold` (v√≠ d·ª•: 0.85 ‚Üí 0.8)

---

## Tham kh·∫£o

- [scikit-learn Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html)
- [Mutual Information](https://en.wikipedia.org/wiki/Mutual_information)
- [Boruta Algorithm](https://www.jstatsoft.org/article/view/v036i11)

